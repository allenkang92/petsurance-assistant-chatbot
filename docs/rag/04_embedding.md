## 3. 임베딩 (Embedding)

### 정의
임베딩은 **RAG 사전 처리의 3번째 단계**로, 텍스트 청크를 **벡터(숫자 배열)**로 변환하는 과정입니다.

```
문서 로드 → 텍스트 분할 → 임베딩 → 벡터 스토어 저장
```

### 임베딩이란?

| 개념 | 설명 |
|:---|:---|
| **벡터** | 텍스트의 의미를 수학적으로 표현한 숫자 배열 |
| **차원** | 벡터를 구성하는 숫자의 개수 (예: 1024차원 = 1024개 숫자) |
| **목적** | LLM이 텍스트를 직접 처리하지 못하므로, 숫자로 변환하여 의미 비교 가능하게 함 |

### 임베딩 과정

```
[텍스트 청크] ──임베딩 모델──▶ [0.12, 0.45, 0.89, ...] (N차원 벡터)
```

> **예시**: "슬개골 탈구 보장" → `[0.12, 0.45, 0.89, ... (1024개)]`

### 질문도 임베딩 필요

| 단계 | 설명 |
|:---|:---|
| 1. 사용자 질문 | "슬개골 탈구 보장되나요?" |
| 2. 질문 임베딩 | 질문을 벡터로 변환 |
| 3. 유사도 계산 | 질문 벡터 ↔ 청크 벡터 비교 |
| 4. 검색 결과 | 가장 유사한 청크 반환 |

### 임베딩 알고리즘 선택

*   알고리즘마다 **생성하는 벡터 차원**이 다름
*   차원이 높으면 세부적 표현 가능, but 항상 성능 향상 X
*   **유사도 계산 결과**도 알고리즘마다 다름
*   **직접 테스트**하며 최적의 알고리즘 선택 필요

### 주요 임베딩 모델 비교

| 모델 | 차원 | 특징 |
|:---|:---:|:---|
| `text-embedding-004` (Google) | 768 | Gemini 생태계, 다국어 지원 |
| `embedding-001` (Google) | 768 | 무료 tier 제한 있음 |
| `ko-sroberta-multitask` (HuggingFace) | 768 | 한국어 특화, 로컬 무료 |
| `text-embedding-3-small` (OpenAI) | 1536 | 빠르고 저렴 |
| `text-embedding-3-large` (OpenAI) | 3072 | 고품질, 비용 높음 |

### 코드 예시

```python
from langchain_community.embeddings import HuggingFaceEmbeddings

# 한국어 특화 임베딩 모델
embeddings = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask",
    model_kwargs={"device": "cpu"}  # GPU: "cuda"
)

# 텍스트 → 벡터 변환
text = "슬개골 탈구 수술비 보장"
vector = embeddings.embed_query(text)
print(f"차원: {len(vector)}")  # 768
print(f"벡터 샘플: {vector[:5]}")  # [0.12, 0.45, ...]
```

---

```
청크(텍스트) ──임베딩──▶ 벡터(숫자) ──저장──▶ 벡터 스토어
                                              ↑
질문(텍스트) ──임베딩──▶ 벡터(숫자) ──비교──▶ 유사도 계산 → 검색 결과
```